{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1452, 0.1372, 0.3195, 0.2089, 0.1893],\n",
      "        [0.1394, 0.1848, 0.2237, 0.1434, 0.3087],\n",
      "        [0.2025, 0.3293, 0.1602, 0.1747, 0.1333]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3,5, requires_grad = True)\n",
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 정답 y \n",
    "y = torch.randint(5, (3,))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change hypothesis to binary prediction by comparing them to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hypothesis 와 같은 사이즈의  빈 배열을 만든다\n",
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5128, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with low-level Cross Entroypy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1,2,1,1],\n",
    "          [2,1,3,2],\n",
    "          [3,1,3,4],\n",
    "          [4,1,5,5],\n",
    "          [1,7,5,5],\n",
    "          [1,2,5,6],\n",
    "          [1,6,6,6],\n",
    "          [1,7,7,7]]\n",
    "y_train = [2,2,2,1,1,1,0,0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 , Cost : 1.0986123085021973\n",
      "Epoch 100/1000 , Cost : 0.8129028081893921\n",
      "Epoch 200/1000 , Cost : 0.780288815498352\n",
      "Epoch 300/1000 , Cost : 0.7650819420814514\n",
      "Epoch 400/1000 , Cost : 0.7557728886604309\n",
      "Epoch 500/1000 , Cost : 0.7492364048957825\n",
      "Epoch 600/1000 , Cost : 0.7443036437034607\n",
      "Epoch 700/1000 , Cost : 0.7404119968414307\n",
      "Epoch 800/1000 , Cost : 0.7372464537620544\n",
      "Epoch 900/1000 , Cost : 0.7346133589744568\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화 \n",
    "W = torch.zeros((4,3), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# optimizer 설정 \n",
    "optimizer = torch.optim.SGD([W,b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs) : \n",
    "    \n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = F.cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # prameter update\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력 \n",
    "    if epoch % 100 == 0 : \n",
    "        print(f'Epoch {epoch}/{nb_epochs} , Cost : {cost.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module) : \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 , Cost : 1.6002955436706543\n",
      "Epoch 100/1000 , Cost : 8.659186363220215\n",
      "Epoch 200/1000 , Cost : 1.7698242664337158\n",
      "Epoch 300/1000 , Cost : 1.6256897449493408\n",
      "Epoch 400/1000 , Cost : 1.8773658275604248\n",
      "Epoch 500/1000 , Cost : 2.0513205528259277\n",
      "Epoch 600/1000 , Cost : 3.097203254699707\n",
      "Epoch 700/1000 , Cost : 1.2486099004745483\n",
      "Epoch 800/1000 , Cost : 0.8728685975074768\n",
      "Epoch 900/1000 , Cost : 0.0035334948915988207\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화 \n",
    "model = SoftmaxClassifierModel()\n",
    "# optimizer 설정 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs) :  \n",
    "    # Cost 계산\n",
    "    hypothesis = model(x_train)\n",
    "    cost = F.cross_entropy(hypothesis, y_train)\n",
    "    \n",
    "    # prameter update\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 100번마다 로그 출력 \n",
    "    if epoch % 100 == 0 : \n",
    "        print(f'Epoch {epoch}/{nb_epochs} , Cost : {cost.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-33.7473,   2.1603,  34.0113],\n",
       "        [-19.8732,   6.8174,  14.5250],\n",
       "        [-62.3554,  29.1691,  37.2332],\n",
       "        [-50.4478,  32.5872,  22.0757],\n",
       "        [  2.4817,   7.1825,  -0.5795],\n",
       "        [  1.8612,  21.5579, -18.9747],\n",
       "        [ 15.9636,  11.2257, -18.9673],\n",
       "        [ 25.5124,  12.7910, -28.6972]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis # 예측값 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7395e-30, 1.4699e-14, 1.0000e+00],\n",
       "        [1.1505e-15, 4.4923e-04, 9.9955e-01],\n",
       "        [5.6052e-44, 3.1454e-04, 9.9969e-01],\n",
       "        [8.6765e-37, 9.9997e-01, 2.7220e-05],\n",
       "        [9.0027e-03, 9.9058e-01, 4.2158e-04],\n",
       "        [2.7913e-09, 1.0000e+00, 2.4940e-18],\n",
       "        [9.9132e-01, 8.6812e-03, 6.6979e-16],\n",
       "        [1.0000e+00, 2.9863e-06, 2.8644e-24]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(hypothesis, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(hypothesis, dim=1).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
