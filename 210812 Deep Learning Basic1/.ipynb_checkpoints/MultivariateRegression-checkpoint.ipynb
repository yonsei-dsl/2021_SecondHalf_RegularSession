{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동미분(Autograd 이해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사하강법 코드를 보고 있으면 requires_grad = True, backward() 등이 나온다. \n",
    "# 이는 파이토치에서 제공하는 자동 미분 기능을 수행하고 있는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 임의의 스칼라 텐서 w 선언\n",
    "# requires_grad = True는 이 텐서에 대한 기울기를 저장하겠다는 의미. 이렇게 하면 'grad'속성에 w에 대한 미분값이 저장됌\n",
    "w = torch.tensor(2.0, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = w**2\n",
    "z = 2*y + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward()를 호출하면 해당 수식에서 requires_grad = True로 설정한 텐서들에 대한 기울기를 계산한다. \n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "print(f'수식을 w로 미분한 값 : {w.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73,80,75],\n",
    "                             [93,88,93],\n",
    "                             [89,91,90],\n",
    "                             [96,98,100],\n",
    "                             [73,66,70]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/100 hypothesis: tensor([0., 0., 0., 0., 0.])  Cost : 29661.800781\n",
      "Epoch    2/100 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605])  Cost : 9298.520508\n",
      "Epoch    3/100 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821])  Cost : 2915.712891\n",
      "Epoch    4/100 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097])  Cost : 915.040527\n",
      "Epoch    5/100 hypothesis: tensor([137.7968, 165.6247, 163.1911, 177.7112, 126.3307])  Cost : 287.936005\n",
      "Epoch    6/100 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891])  Cost : 91.371010\n",
      "Epoch    7/100 hypothesis: tensor([148.1035, 178.0144, 175.3980, 191.0042, 135.7812])  Cost : 29.758139\n",
      "Epoch    8/100 hypothesis: tensor([150.1744, 180.5042, 177.8508, 193.6753, 137.6805])  Cost : 10.445305\n",
      "Epoch    9/100 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440])  Cost : 4.391228\n",
      "Epoch   10/100 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396])  Cost : 2.493135\n",
      "Epoch   11/100 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732])  Cost : 1.897688\n",
      "Epoch   12/100 hypothesis: tensor([152.5485, 183.3610, 180.6640, 196.7389, 139.8602])  Cost : 1.710541\n",
      "Epoch   13/100 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651])  Cost : 1.651412\n",
      "Epoch   14/100 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240])  Cost : 1.632387\n",
      "Epoch   15/100 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571])  Cost : 1.625923\n",
      "Epoch   16/100 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759])  Cost : 1.623412\n",
      "Epoch   17/100 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865])  Cost : 1.622141\n",
      "Epoch   18/100 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927])  Cost : 1.621253\n",
      "Epoch   19/100 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0662, 140.0963])  Cost : 1.620500\n",
      "Epoch   20/100 hypothesis: tensor([152.8014, 183.6715, 180.9666, 197.0686, 140.0985])  Cost : 1.619770\n",
      "Epoch   21/100 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.1000])  Cost : 1.619033\n",
      "Epoch   22/100 hypothesis: tensor([152.8022, 183.6741, 180.9683, 197.0706, 140.1009])  Cost : 1.618346\n",
      "Epoch   23/100 hypothesis: tensor([152.8021, 183.6749, 180.9686, 197.0709, 140.1016])  Cost : 1.617637\n",
      "Epoch   24/100 hypothesis: tensor([152.8019, 183.6754, 180.9687, 197.0710, 140.1022])  Cost : 1.616930\n",
      "Epoch   25/100 hypothesis: tensor([152.8016, 183.6758, 180.9687, 197.0711, 140.1027])  Cost : 1.616221\n",
      "Epoch   26/100 hypothesis: tensor([152.8012, 183.6762, 180.9686, 197.0710, 140.1032])  Cost : 1.615508\n",
      "Epoch   27/100 hypothesis: tensor([152.8008, 183.6765, 180.9686, 197.0710, 140.1036])  Cost : 1.614815\n",
      "Epoch   28/100 hypothesis: tensor([152.8004, 183.6768, 180.9684, 197.0709, 140.1041])  Cost : 1.614109\n",
      "Epoch   29/100 hypothesis: tensor([152.8000, 183.6772, 180.9683, 197.0707, 140.1045])  Cost : 1.613387\n",
      "Epoch   30/100 hypothesis: tensor([152.7995, 183.6775, 180.9682, 197.0706, 140.1049])  Cost : 1.612701\n",
      "Epoch   31/100 hypothesis: tensor([152.7991, 183.6778, 180.9681, 197.0705, 140.1053])  Cost : 1.611987\n",
      "Epoch   32/100 hypothesis: tensor([152.7987, 183.6781, 180.9679, 197.0704, 140.1057])  Cost : 1.611289\n",
      "Epoch   33/100 hypothesis: tensor([152.7982, 183.6784, 180.9678, 197.0703, 140.1061])  Cost : 1.610588\n",
      "Epoch   34/100 hypothesis: tensor([152.7978, 183.6787, 180.9677, 197.0702, 140.1065])  Cost : 1.609874\n",
      "Epoch   35/100 hypothesis: tensor([152.7974, 183.6790, 180.9676, 197.0701, 140.1069])  Cost : 1.609183\n",
      "Epoch   36/100 hypothesis: tensor([152.7969, 183.6793, 180.9674, 197.0700, 140.1073])  Cost : 1.608466\n",
      "Epoch   37/100 hypothesis: tensor([152.7965, 183.6796, 180.9673, 197.0699, 140.1078])  Cost : 1.607776\n",
      "Epoch   38/100 hypothesis: tensor([152.7961, 183.6799, 180.9672, 197.0698, 140.1082])  Cost : 1.607080\n",
      "Epoch   39/100 hypothesis: tensor([152.7957, 183.6802, 180.9670, 197.0697, 140.1086])  Cost : 1.606384\n",
      "Epoch   40/100 hypothesis: tensor([152.7952, 183.6805, 180.9669, 197.0695, 140.1090])  Cost : 1.605665\n",
      "Epoch   41/100 hypothesis: tensor([152.7948, 183.6807, 180.9668, 197.0694, 140.1094])  Cost : 1.604974\n",
      "Epoch   42/100 hypothesis: tensor([152.7944, 183.6810, 180.9666, 197.0693, 140.1098])  Cost : 1.604266\n",
      "Epoch   43/100 hypothesis: tensor([152.7939, 183.6813, 180.9665, 197.0692, 140.1102])  Cost : 1.603586\n",
      "Epoch   44/100 hypothesis: tensor([152.7935, 183.6816, 180.9664, 197.0691, 140.1106])  Cost : 1.602878\n",
      "Epoch   45/100 hypothesis: tensor([152.7931, 183.6819, 180.9663, 197.0690, 140.1110])  Cost : 1.602179\n",
      "Epoch   46/100 hypothesis: tensor([152.7926, 183.6822, 180.9661, 197.0689, 140.1114])  Cost : 1.601486\n",
      "Epoch   47/100 hypothesis: tensor([152.7922, 183.6826, 180.9660, 197.0688, 140.1118])  Cost : 1.600765\n",
      "Epoch   48/100 hypothesis: tensor([152.7918, 183.6828, 180.9659, 197.0686, 140.1122])  Cost : 1.600072\n",
      "Epoch   49/100 hypothesis: tensor([152.7914, 183.6831, 180.9657, 197.0685, 140.1126])  Cost : 1.599369\n",
      "Epoch   50/100 hypothesis: tensor([152.7909, 183.6834, 180.9656, 197.0684, 140.1130])  Cost : 1.598692\n",
      "Epoch   51/100 hypothesis: tensor([152.7905, 183.6837, 180.9655, 197.0683, 140.1134])  Cost : 1.597991\n",
      "Epoch   52/100 hypothesis: tensor([152.7900, 183.6840, 180.9653, 197.0682, 140.1138])  Cost : 1.597289\n",
      "Epoch   53/100 hypothesis: tensor([152.7896, 183.6843, 180.9652, 197.0681, 140.1143])  Cost : 1.596590\n",
      "Epoch   54/100 hypothesis: tensor([152.7892, 183.6846, 180.9651, 197.0679, 140.1147])  Cost : 1.595898\n",
      "Epoch   55/100 hypothesis: tensor([152.7888, 183.6849, 180.9650, 197.0678, 140.1151])  Cost : 1.595198\n",
      "Epoch   56/100 hypothesis: tensor([152.7883, 183.6852, 180.9648, 197.0677, 140.1155])  Cost : 1.594514\n",
      "Epoch   57/100 hypothesis: tensor([152.7879, 183.6855, 180.9647, 197.0676, 140.1159])  Cost : 1.593821\n",
      "Epoch   58/100 hypothesis: tensor([152.7875, 183.6858, 180.9646, 197.0675, 140.1163])  Cost : 1.593129\n",
      "Epoch   59/100 hypothesis: tensor([152.7870, 183.6861, 180.9644, 197.0674, 140.1167])  Cost : 1.592433\n",
      "Epoch   60/100 hypothesis: tensor([152.7866, 183.6864, 180.9643, 197.0673, 140.1171])  Cost : 1.591737\n",
      "Epoch   61/100 hypothesis: tensor([152.7862, 183.6867, 180.9642, 197.0672, 140.1175])  Cost : 1.591038\n",
      "Epoch   62/100 hypothesis: tensor([152.7858, 183.6870, 180.9641, 197.0670, 140.1179])  Cost : 1.590336\n",
      "Epoch   63/100 hypothesis: tensor([152.7853, 183.6873, 180.9639, 197.0669, 140.1183])  Cost : 1.589665\n",
      "Epoch   64/100 hypothesis: tensor([152.7849, 183.6876, 180.9638, 197.0668, 140.1187])  Cost : 1.588963\n",
      "Epoch   65/100 hypothesis: tensor([152.7845, 183.6879, 180.9637, 197.0667, 140.1191])  Cost : 1.588273\n",
      "Epoch   66/100 hypothesis: tensor([152.7840, 183.6882, 180.9635, 197.0666, 140.1195])  Cost : 1.587576\n",
      "Epoch   67/100 hypothesis: tensor([152.7836, 183.6885, 180.9634, 197.0665, 140.1199])  Cost : 1.586890\n",
      "Epoch   68/100 hypothesis: tensor([152.7832, 183.6888, 180.9633, 197.0664, 140.1203])  Cost : 1.586196\n",
      "Epoch   69/100 hypothesis: tensor([152.7828, 183.6890, 180.9632, 197.0663, 140.1207])  Cost : 1.585522\n",
      "Epoch   70/100 hypothesis: tensor([152.7823, 183.6893, 180.9630, 197.0661, 140.1211])  Cost : 1.584821\n",
      "Epoch   71/100 hypothesis: tensor([152.7819, 183.6896, 180.9629, 197.0660, 140.1215])  Cost : 1.584131\n",
      "Epoch   72/100 hypothesis: tensor([152.7815, 183.6899, 180.9628, 197.0659, 140.1219])  Cost : 1.583444\n",
      "Epoch   73/100 hypothesis: tensor([152.7810, 183.6902, 180.9626, 197.0658, 140.1223])  Cost : 1.582750\n",
      "Epoch   74/100 hypothesis: tensor([152.7806, 183.6905, 180.9625, 197.0657, 140.1227])  Cost : 1.582072\n",
      "Epoch   75/100 hypothesis: tensor([152.7802, 183.6908, 180.9624, 197.0656, 140.1231])  Cost : 1.581385\n",
      "Epoch   76/100 hypothesis: tensor([152.7798, 183.6911, 180.9622, 197.0655, 140.1236])  Cost : 1.580690\n",
      "Epoch   77/100 hypothesis: tensor([152.7793, 183.6914, 180.9621, 197.0654, 140.1240])  Cost : 1.579996\n",
      "Epoch   78/100 hypothesis: tensor([152.7789, 183.6917, 180.9620, 197.0653, 140.1244])  Cost : 1.579299\n",
      "Epoch   79/100 hypothesis: tensor([152.7785, 183.6920, 180.9619, 197.0651, 140.1248])  Cost : 1.578632\n",
      "Epoch   80/100 hypothesis: tensor([152.7781, 183.6923, 180.9617, 197.0650, 140.1252])  Cost : 1.577947\n",
      "Epoch   81/100 hypothesis: tensor([152.7776, 183.6926, 180.9616, 197.0649, 140.1256])  Cost : 1.577271\n",
      "Epoch   82/100 hypothesis: tensor([152.7772, 183.6929, 180.9615, 197.0648, 140.1260])  Cost : 1.576558\n",
      "Epoch   83/100 hypothesis: tensor([152.7768, 183.6932, 180.9614, 197.0647, 140.1264])  Cost : 1.575896\n",
      "Epoch   84/100 hypothesis: tensor([152.7764, 183.6935, 180.9612, 197.0646, 140.1268])  Cost : 1.575205\n",
      "Epoch   85/100 hypothesis: tensor([152.7759, 183.6938, 180.9611, 197.0645, 140.1272])  Cost : 1.574519\n",
      "Epoch   86/100 hypothesis: tensor([152.7755, 183.6940, 180.9610, 197.0643, 140.1276])  Cost : 1.573839\n",
      "Epoch   87/100 hypothesis: tensor([152.7751, 183.6944, 180.9609, 197.0643, 140.1280])  Cost : 1.573149\n",
      "Epoch   88/100 hypothesis: tensor([152.7747, 183.6947, 180.9607, 197.0641, 140.1284])  Cost : 1.572457\n",
      "Epoch   89/100 hypothesis: tensor([152.7742, 183.6949, 180.9606, 197.0640, 140.1288])  Cost : 1.571791\n",
      "Epoch   90/100 hypothesis: tensor([152.7738, 183.6952, 180.9605, 197.0639, 140.1292])  Cost : 1.571114\n",
      "Epoch   91/100 hypothesis: tensor([152.7734, 183.6955, 180.9603, 197.0638, 140.1296])  Cost : 1.570435\n",
      "Epoch   92/100 hypothesis: tensor([152.7729, 183.6958, 180.9602, 197.0637, 140.1300])  Cost : 1.569754\n",
      "Epoch   93/100 hypothesis: tensor([152.7725, 183.6961, 180.9601, 197.0636, 140.1304])  Cost : 1.569064\n",
      "Epoch   94/100 hypothesis: tensor([152.7721, 183.6964, 180.9600, 197.0634, 140.1308])  Cost : 1.568386\n",
      "Epoch   95/100 hypothesis: tensor([152.7717, 183.6967, 180.9598, 197.0633, 140.1312])  Cost : 1.567708\n",
      "Epoch   96/100 hypothesis: tensor([152.7713, 183.6970, 180.9597, 197.0632, 140.1316])  Cost : 1.567021\n",
      "Epoch   97/100 hypothesis: tensor([152.7708, 183.6973, 180.9596, 197.0631, 140.1320])  Cost : 1.566340\n",
      "Epoch   98/100 hypothesis: tensor([152.7704, 183.6976, 180.9594, 197.0630, 140.1324])  Cost : 1.565658\n",
      "Epoch   99/100 hypothesis: tensor([152.7700, 183.6979, 180.9593, 197.0629, 140.1328])  Cost : 1.564987\n",
      "Epoch  100/100 hypothesis: tensor([152.7695, 183.6982, 180.9592, 197.0628, 140.1332])  Cost : 1.564298\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화 \n",
    "W = torch.zeros((3,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = torch.optim.SGD([W,b],lr = 1e-5)     ## learning rate에 따라 발산할 수 있다. lr = 0.01만 돼도 발산한다. \n",
    "nb_epochs = 100\n",
    "\n",
    "for epoch in range(nb_epochs) : \n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {}  Cost : {:.6f}'.format(epoch+1, nb_epochs,\n",
    "                                                               hypothesis.squeeze().detach(),\n",
    "                                                               cost.item()\n",
    "                                                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.5102],\n",
       "        [ 6.0929],\n",
       "        [-0.5702]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6735],\n",
       "        [0.6610],\n",
       "        [0.6762]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch가 제공하는 nn.Module을 사용한 모델 구현\n",
    "### 대부분의 파이토치 구현이 다음 과정을 따른다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module) :\n",
    "    def __init__(self) : # 여기서 모델의 구조를 정의하는 생성자를 정의한다. \n",
    "        super().__init__()   # (above) 이는 객체가 갖는 속성값을 초기화 하는 역할로, 객체가 생성될때 자동으로 호출됌.\n",
    "        self.linear = nn.Linear(3,1)  # 입력차원 3, 출력차원 1 \n",
    "        \n",
    "    def forward(self, x) :      # forward 함수는 모델이 학습데이터를 입력받아서 forward연산 (예측)을 진행하는 함수\n",
    "        return self.linear(x)        # 모델 객체를 데이터와 함께 호출하면 자동으로 실행됌. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.3791, -0.1642, -0.5551]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1892], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 모델에 저장되어 있는 파라미터 확인 \n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1/100 hypothesis: tensor([-8.2666, -2.7712, -6.4352, -6.7662, -0.5484])  Cost : 31427.130859\n",
      "Epoch    2/100 hypothesis: tensor([60.9637, 80.4374, 75.5522, 82.5160, 62.9186])  Cost : 9852.556641\n",
      "Epoch    3/100 hypothesis: tensor([ 99.7235, 127.0226, 121.4540, 132.5018,  98.4512])  Cost : 3090.071289\n",
      "Epoch    4/100 hypothesis: tensor([121.4240, 153.1038, 147.1528, 160.4871, 118.3443])  Cost : 970.390503\n",
      "Epoch    5/100 hypothesis: tensor([133.5735, 167.7055, 161.5408, 176.1551, 129.4816])  Cost : 305.981445\n",
      "Epoch    6/100 hypothesis: tensor([140.3759, 175.8803, 169.5961, 184.9271, 135.7167])  Cost : 97.723663\n",
      "Epoch    7/100 hypothesis: tensor([144.1845, 180.4569, 174.1061, 189.8383, 139.2072])  Cost : 32.445244\n",
      "Epoch    8/100 hypothesis: tensor([146.3171, 183.0190, 176.6312, 192.5879, 141.1612])  Cost : 11.982953\n",
      "Epoch    9/100 hypothesis: tensor([147.5114, 184.4532, 178.0449, 194.1274, 142.2549])  Cost : 5.568195\n",
      "Epoch   10/100 hypothesis: tensor([148.1802, 185.2560, 178.8365, 194.9893, 142.8671])  Cost : 3.556630\n",
      "Epoch   11/100 hypothesis: tensor([148.5550, 185.7053, 179.2798, 195.4720, 143.2095])  Cost : 2.925239\n",
      "Epoch   12/100 hypothesis: tensor([148.7650, 185.9566, 179.5280, 195.7422, 143.4010])  Cost : 2.726429\n",
      "Epoch   13/100 hypothesis: tensor([148.8829, 186.0972, 179.6671, 195.8936, 143.5080])  Cost : 2.663235\n",
      "Epoch   14/100 hypothesis: tensor([148.9492, 186.1756, 179.7451, 195.9784, 143.5677])  Cost : 2.642549\n",
      "Epoch   15/100 hypothesis: tensor([148.9865, 186.2194, 179.7888, 196.0259, 143.6008])  Cost : 2.635182\n",
      "Epoch   16/100 hypothesis: tensor([149.0077, 186.2437, 179.8134, 196.0526, 143.6192])  Cost : 2.631991\n",
      "Epoch   17/100 hypothesis: tensor([149.0199, 186.2572, 179.8272, 196.0675, 143.6292])  Cost : 2.630104\n",
      "Epoch   18/100 hypothesis: tensor([149.0269, 186.2645, 179.8350, 196.0760, 143.6346])  Cost : 2.628624\n",
      "Epoch   19/100 hypothesis: tensor([149.0311, 186.2684, 179.8395, 196.0807, 143.6374])  Cost : 2.627300\n",
      "Epoch   20/100 hypothesis: tensor([149.0338, 186.2704, 179.8420, 196.0835, 143.6387])  Cost : 2.625985\n",
      "Epoch   21/100 hypothesis: tensor([149.0355, 186.2714, 179.8436, 196.0851, 143.6392])  Cost : 2.624715\n",
      "Epoch   22/100 hypothesis: tensor([149.0367, 186.2718, 179.8445, 196.0860, 143.6393])  Cost : 2.623406\n",
      "Epoch   23/100 hypothesis: tensor([149.0377, 186.2718, 179.8451, 196.0866, 143.6391])  Cost : 2.622144\n",
      "Epoch   24/100 hypothesis: tensor([149.0385, 186.2716, 179.8455, 196.0869, 143.6387])  Cost : 2.620870\n",
      "Epoch   25/100 hypothesis: tensor([149.0392, 186.2713, 179.8459, 196.0872, 143.6383])  Cost : 2.619572\n",
      "Epoch   26/100 hypothesis: tensor([149.0399, 186.2710, 179.8461, 196.0874, 143.6378])  Cost : 2.618312\n",
      "Epoch   27/100 hypothesis: tensor([149.0405, 186.2706, 179.8463, 196.0876, 143.6373])  Cost : 2.617032\n",
      "Epoch   28/100 hypothesis: tensor([149.0411, 186.2702, 179.8466, 196.0877, 143.6368])  Cost : 2.615750\n",
      "Epoch   29/100 hypothesis: tensor([149.0417, 186.2698, 179.8468, 196.0879, 143.6363])  Cost : 2.614471\n",
      "Epoch   30/100 hypothesis: tensor([149.0423, 186.2694, 179.8469, 196.0880, 143.6358])  Cost : 2.613211\n",
      "Epoch   31/100 hypothesis: tensor([149.0430, 186.2690, 179.8471, 196.0881, 143.6353])  Cost : 2.611922\n",
      "Epoch   32/100 hypothesis: tensor([149.0436, 186.2686, 179.8473, 196.0882, 143.6347])  Cost : 2.610653\n",
      "Epoch   33/100 hypothesis: tensor([149.0442, 186.2682, 179.8475, 196.0883, 143.6342])  Cost : 2.609383\n",
      "Epoch   34/100 hypothesis: tensor([149.0448, 186.2678, 179.8477, 196.0885, 143.6337])  Cost : 2.608114\n",
      "Epoch   35/100 hypothesis: tensor([149.0454, 186.2674, 179.8479, 196.0886, 143.6331])  Cost : 2.606838\n",
      "Epoch   36/100 hypothesis: tensor([149.0460, 186.2670, 179.8481, 196.0887, 143.6326])  Cost : 2.605562\n",
      "Epoch   37/100 hypothesis: tensor([149.0466, 186.2665, 179.8483, 196.0888, 143.6321])  Cost : 2.604295\n",
      "Epoch   38/100 hypothesis: tensor([149.0472, 186.2661, 179.8484, 196.0890, 143.6316])  Cost : 2.603020\n",
      "Epoch   39/100 hypothesis: tensor([149.0478, 186.2657, 179.8486, 196.0891, 143.6310])  Cost : 2.601763\n",
      "Epoch   40/100 hypothesis: tensor([149.0484, 186.2653, 179.8488, 196.0892, 143.6305])  Cost : 2.600497\n",
      "Epoch   41/100 hypothesis: tensor([149.0490, 186.2649, 179.8490, 196.0893, 143.6300])  Cost : 2.599229\n",
      "Epoch   42/100 hypothesis: tensor([149.0496, 186.2645, 179.8492, 196.0894, 143.6295])  Cost : 2.597964\n",
      "Epoch   43/100 hypothesis: tensor([149.0502, 186.2641, 179.8494, 196.0896, 143.6290])  Cost : 2.596697\n",
      "Epoch   44/100 hypothesis: tensor([149.0508, 186.2637, 179.8496, 196.0897, 143.6284])  Cost : 2.595417\n",
      "Epoch   45/100 hypothesis: tensor([149.0514, 186.2633, 179.8498, 196.0898, 143.6279])  Cost : 2.594151\n",
      "Epoch   46/100 hypothesis: tensor([149.0520, 186.2629, 179.8499, 196.0899, 143.6274])  Cost : 2.592906\n",
      "Epoch   47/100 hypothesis: tensor([149.0526, 186.2625, 179.8501, 196.0900, 143.6268])  Cost : 2.591632\n",
      "Epoch   48/100 hypothesis: tensor([149.0532, 186.2620, 179.8503, 196.0902, 143.6263])  Cost : 2.590369\n",
      "Epoch   49/100 hypothesis: tensor([149.0538, 186.2616, 179.8505, 196.0903, 143.6258])  Cost : 2.589106\n",
      "Epoch   50/100 hypothesis: tensor([149.0544, 186.2612, 179.8507, 196.0904, 143.6253])  Cost : 2.587834\n",
      "Epoch   51/100 hypothesis: tensor([149.0550, 186.2608, 179.8509, 196.0905, 143.6247])  Cost : 2.586581\n",
      "Epoch   52/100 hypothesis: tensor([149.0556, 186.2604, 179.8511, 196.0906, 143.6242])  Cost : 2.585318\n",
      "Epoch   53/100 hypothesis: tensor([149.0562, 186.2600, 179.8512, 196.0907, 143.6237])  Cost : 2.584074\n",
      "Epoch   54/100 hypothesis: tensor([149.0568, 186.2596, 179.8514, 196.0909, 143.6232])  Cost : 2.582813\n",
      "Epoch   55/100 hypothesis: tensor([149.0574, 186.2592, 179.8516, 196.0910, 143.6227])  Cost : 2.581553\n",
      "Epoch   56/100 hypothesis: tensor([149.0580, 186.2588, 179.8518, 196.0911, 143.6221])  Cost : 2.580300\n",
      "Epoch   57/100 hypothesis: tensor([149.0586, 186.2583, 179.8520, 196.0912, 143.6216])  Cost : 2.579032\n",
      "Epoch   58/100 hypothesis: tensor([149.0592, 186.2579, 179.8522, 196.0913, 143.6211])  Cost : 2.577788\n",
      "Epoch   59/100 hypothesis: tensor([149.0598, 186.2576, 179.8524, 196.0915, 143.6206])  Cost : 2.576538\n",
      "Epoch   60/100 hypothesis: tensor([149.0604, 186.2571, 179.8525, 196.0916, 143.6200])  Cost : 2.575261\n",
      "Epoch   61/100 hypothesis: tensor([149.0610, 186.2567, 179.8527, 196.0917, 143.6195])  Cost : 2.574018\n",
      "Epoch   62/100 hypothesis: tensor([149.0616, 186.2563, 179.8529, 196.0918, 143.6190])  Cost : 2.572770\n",
      "Epoch   63/100 hypothesis: tensor([149.0622, 186.2559, 179.8531, 196.0919, 143.6185])  Cost : 2.571520\n",
      "Epoch   64/100 hypothesis: tensor([149.0628, 186.2555, 179.8533, 196.0921, 143.6179])  Cost : 2.570269\n",
      "Epoch   65/100 hypothesis: tensor([149.0634, 186.2551, 179.8535, 196.0922, 143.6174])  Cost : 2.568996\n",
      "Epoch   66/100 hypothesis: tensor([149.0640, 186.2547, 179.8537, 196.0923, 143.6169])  Cost : 2.567756\n",
      "Epoch   67/100 hypothesis: tensor([149.0646, 186.2543, 179.8538, 196.0924, 143.6164])  Cost : 2.566509\n",
      "Epoch   68/100 hypothesis: tensor([149.0652, 186.2539, 179.8540, 196.0925, 143.6158])  Cost : 2.565262\n",
      "Epoch   69/100 hypothesis: tensor([149.0658, 186.2535, 179.8542, 196.0927, 143.6153])  Cost : 2.564023\n",
      "Epoch   70/100 hypothesis: tensor([149.0663, 186.2530, 179.8544, 196.0928, 143.6148])  Cost : 2.562758\n",
      "Epoch   71/100 hypothesis: tensor([149.0669, 186.2526, 179.8546, 196.0929, 143.6143])  Cost : 2.561520\n",
      "Epoch   72/100 hypothesis: tensor([149.0676, 186.2522, 179.8548, 196.0930, 143.6138])  Cost : 2.560265\n",
      "Epoch   73/100 hypothesis: tensor([149.0681, 186.2518, 179.8549, 196.0931, 143.6132])  Cost : 2.559026\n",
      "Epoch   74/100 hypothesis: tensor([149.0687, 186.2514, 179.8551, 196.0932, 143.6127])  Cost : 2.557782\n",
      "Epoch   75/100 hypothesis: tensor([149.0693, 186.2510, 179.8553, 196.0934, 143.6122])  Cost : 2.556536\n",
      "Epoch   76/100 hypothesis: tensor([149.0699, 186.2506, 179.8555, 196.0935, 143.6117])  Cost : 2.555299\n",
      "Epoch   77/100 hypothesis: tensor([149.0705, 186.2502, 179.8557, 196.0936, 143.6112])  Cost : 2.554055\n",
      "Epoch   78/100 hypothesis: tensor([149.0711, 186.2498, 179.8559, 196.0937, 143.6106])  Cost : 2.552802\n",
      "Epoch   79/100 hypothesis: tensor([149.0717, 186.2494, 179.8560, 196.0938, 143.6101])  Cost : 2.551558\n",
      "Epoch   80/100 hypothesis: tensor([149.0723, 186.2490, 179.8562, 196.0939, 143.6096])  Cost : 2.550315\n",
      "Epoch   81/100 hypothesis: tensor([149.0729, 186.2486, 179.8564, 196.0941, 143.6091])  Cost : 2.549072\n",
      "Epoch   82/100 hypothesis: tensor([149.0735, 186.2482, 179.8566, 196.0942, 143.6086])  Cost : 2.547856\n",
      "Epoch   83/100 hypothesis: tensor([149.0741, 186.2478, 179.8568, 196.0943, 143.6080])  Cost : 2.546621\n",
      "Epoch   84/100 hypothesis: tensor([149.0747, 186.2473, 179.8570, 196.0944, 143.6075])  Cost : 2.545362\n",
      "Epoch   85/100 hypothesis: tensor([149.0753, 186.2469, 179.8571, 196.0945, 143.6070])  Cost : 2.544111\n",
      "Epoch   86/100 hypothesis: tensor([149.0759, 186.2466, 179.8573, 196.0947, 143.6065])  Cost : 2.542887\n",
      "Epoch   87/100 hypothesis: tensor([149.0765, 186.2461, 179.8575, 196.0948, 143.6060])  Cost : 2.541646\n",
      "Epoch   88/100 hypothesis: tensor([149.0771, 186.2457, 179.8577, 196.0949, 143.6054])  Cost : 2.540414\n",
      "Epoch   89/100 hypothesis: tensor([149.0777, 186.2453, 179.8579, 196.0950, 143.6049])  Cost : 2.539181\n",
      "Epoch   90/100 hypothesis: tensor([149.0782, 186.2449, 179.8581, 196.0951, 143.6044])  Cost : 2.537950\n",
      "Epoch   91/100 hypothesis: tensor([149.0788, 186.2445, 179.8582, 196.0952, 143.6039])  Cost : 2.536721\n",
      "Epoch   92/100 hypothesis: tensor([149.0794, 186.2441, 179.8584, 196.0954, 143.6033])  Cost : 2.535471\n",
      "Epoch   93/100 hypothesis: tensor([149.0800, 186.2437, 179.8586, 196.0955, 143.6028])  Cost : 2.534267\n",
      "Epoch   94/100 hypothesis: tensor([149.0806, 186.2433, 179.8588, 196.0956, 143.6023])  Cost : 2.533020\n",
      "Epoch   95/100 hypothesis: tensor([149.0812, 186.2429, 179.8590, 196.0957, 143.6018])  Cost : 2.531781\n",
      "Epoch   96/100 hypothesis: tensor([149.0818, 186.2425, 179.8592, 196.0958, 143.6013])  Cost : 2.530561\n",
      "Epoch   97/100 hypothesis: tensor([149.0824, 186.2421, 179.8593, 196.0959, 143.6008])  Cost : 2.529332\n",
      "Epoch   98/100 hypothesis: tensor([149.0830, 186.2417, 179.8595, 196.0961, 143.6003])  Cost : 2.528102\n",
      "Epoch   99/100 hypothesis: tensor([149.0836, 186.2413, 179.8597, 196.0962, 143.5997])  Cost : 2.526858\n",
      "Epoch  100/100 hypothesis: tensor([149.0842, 186.2409, 179.8599, 196.0963, 143.5992])  Cost : 2.525647\n"
     ]
    }
   ],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.00001)  \n",
    "## lr은 중요한 하이퍼 파라미터. 0.01, 0.001, 0.00001을 넣어보고 비교해봐라\n",
    "\n",
    "for epoch in range(nb_epochs) : \n",
    "    # hypothesis 계산\n",
    "    hypothesis = model(x_train)  \n",
    "        \n",
    "    # cost 계산 \n",
    "    cost = torch.nn.functional.mse_loss(hypothesis, y_train)   # 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "    \n",
    "    \n",
    "    # cost로 hypothesis 개선 \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Epoch {:4d}/{} hypothesis: {}  Cost : {:.6f}'.format(epoch+1, nb_epochs, hypothesis.squeeze().detach(),\n",
    "                                                               cost.item()\n",
    "                                                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
